{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "KXMUivQv1C_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   - A parameter is like a placeholder or input that you give to a function, method, or procedure so it can work with different values without rewriting the code each time.\n",
        "2. What is correlation? What does negative correlation mean?\n",
        "   - Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It tells us whether and how strongly changes in one variable are associated with changes in another. A negative correlation means that as one variable increases, the other decreases, and vice versa. For example, the relationship between the price of a product and its demand is usually negative—when prices rise, demand tends to fall. The value of correlation ranges from -1 to +1, where -1 indicates a perfect negative correlation, 0 indicates no correlation, and +1 indicates a perfect positive correlation.\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "   - Machine Learning (ML) is a branch of Artificial Intelligence (AI) that focuses on creating systems capable of learning from data and improving their performance over time without being explicitly programmed. Instead of following fixed rules, machine learning models identify patterns in data and make predictions or decisions based on those patterns.\n",
        "   The main components of Machine Learning are:\n",
        "   Data – The raw information (numbers, text, images, etc.) used to train and evaluate the model.\n",
        "   Features – The measurable properties or characteristics of the data that the model uses to make predictions.\n",
        "   Model – The mathematical or computational structure (like decision trees, neural networks, or regression models) that learns patterns from data.Training – The process of feeding data into the model so it can learn the relationship between input (features) and output (labels or targets)\n",
        "   Evaluation – Testing the model with new, unseen data to check how well it performs.\n",
        "   Prediction – The outcome or result produced by the trained model when it encounters new data.\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "   - In Machine Learning, the loss value is a numerical measure of how far off the model’s predictions are from the actual correct values. It shows the “error” made by the model: a high loss means the model’s predictions are poor, while a low loss means the model is predicting more accurately. During training, the goal is to minimize this loss value by adjusting the model’s parameters so that its predictions get closer to the real outcomes. If the loss value decreases consistently over time, it indicates the model is learning well. However, if the loss remains high or stops improving, it suggests the model may not be good enough, might be underfitting, or could be overfitting the data. In short, the loss value acts like a guide to judge whether a model is performing well or needs improvement.\n",
        "5. What are continuous and categorical variables?\n",
        "   - A continuous variable is a type of numerical variable that can take an infinite number of values within a given range. These values are measurable and can include fractions or decimals. For example, height, weight, temperature, and time are continuous variables because they can be measured with high precision (e.g., 5.62 feet, 72.3 kg).\n",
        "   A categorical variable, on the other hand, represents distinct groups or categories and cannot be measured on a numerical scale in a meaningful way. These variables describe qualities or characteristics and can be divided into subtypes like nominal (no natural order, e.g., colors: red, blue, green) and ordinal (with a meaningful order, e.g., education levels: high school, graduate, postgraduate).\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "   - In Machine Learning, categorical variables need to be converted into numerical form since most algorithms work with numbers. This process is called encoding, and there are several common techniques to handle it. Label Encoding assigns each category a unique number, though it may create a false sense of order. One-Hot Encoding creates new binary columns for each category, marking presence with 1 and absence with 0, which avoids ordering but increases data size when categories are many. Ordinal Encoding is used when categories have a natural order, such as education levels, and maps them to integers that respect that order. Frequency or Count Encoding replaces categories with the number of times they appear in the dataset, while Target Encoding substitutes categories with the mean of the target variable for that category, which is powerful but can cause overfitting if not handled carefully. The choice of technique depends on whether the categories are ordered and how many unique values they contain.\n",
        "7. What do you mean by training and testing a dataset?\n",
        "   - Training and testing a dataset refers to the process of splitting data into two parts so that a Machine Learning model can learn from one part and be evaluated on the other.The training dataset is the portion of data used to teach the model by allowing it to find patterns and relationships between input features and the output. During this phase, the model adjusts its internal parameters to minimize errors. The testing dataset, on the other hand, is a separate portion of data that the model has never seen before. It is used to check how well the model performs on new, unseen data, which helps us measure its accuracy, reliability, and ability to generalize.\n",
        "8. What is sklearn.preprocessing?\n",
        "   - sklearn.preprocessing is a module in Scikit-Learn that provides various tools to prepare and transform raw data into a suitable format for Machine Learning models. Since real-world data often comes in different scales, formats, or types, preprocessing is essential to ensure better model performance. This module includes techniques such as scaling numerical features to a common range using methods like StandardScaler or MinMaxScaler, normalization to make each feature contribute equally, encoding categorical variables using tools like LabelEncoder and OneHotEncoder, binarization to convert values into 0 or 1 based on a threshold, and feature generation with PolynomialFeatures to create new input variables. In short, sklearn.preprocessing helps clean, scale, and encode data so that machine learning algorithms can work more effectively and produce accurate results.\n",
        "9. What is a Test set?\n",
        "   - A test set is a portion of the dataset that is kept aside and not used while training a Machine Learning model. Its main purpose is to evaluate how well the trained model performs on new, unseen data. By testing the model on this separate set, we can check its accuracy, reliability, and ability to generalize beyond the training examples. In short, the test set acts as a final exam for the model, showing whether it has truly learned patterns from the data or if it has simply memorized the training examples.\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "    - In Python, data is usually split into training and testing sets using Scikit-Learn’s train_test_split function, where a portion of the data (commonly 70–80%) is used for training the model and the remaining (20–30%) is used for testing its performance. For example, train_test_split(X, y, test_size=0.2, random_state=42) divides the dataset so the model learns patterns from the training data and is then evaluated on unseen test data. When approaching a Machine Learning problem, the process generally involves first understanding the problem and defining the goal, then collecting and exploring data to handle missing values, scaling, or encoding categorical features. After preparing the data, it is split into training and testing sets, and a suitable model is chosen and trained. The trained model is evaluated using metrics like accuracy, precision, or recall, and then fine-tuned through hyperparameter optimization. Finally, the model is deployed for real-world use and continuously monitored to ensure it performs well as new data becomes available.\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    - We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the dataset and prepare it properly for machine learning. Real-world data often contains issues like missing values, duplicates, outliers, or inconsistent formats, which can mislead the model if not handled in advance. Through EDA, we can summarize key characteristics of the data, visualize distributions, and identify relationships or correlations between variables. It also helps in selecting the right features, deciding whether scaling or encoding is required, and spotting potential data leakage. In short, EDA ensures data quality, provides insights for better feature engineering, and prevents mistakes, making the model more reliable and accurate.\n",
        "12. What is correlation?\n",
        "    - Correlation is a statistical measure that shows the strength and direction of the relationship between two variables. It tells us how closely changes in one variable are associated with changes in another. The correlation value ranges from -1 to +1, where +1 indicates a perfect positive correlation (both variables increase or decrease together), -1 indicates a perfect negative correlation (one variable increases while the other decreases), and 0 means there is no relationship between the variables. For example, height and weight often show a positive correlation, while the price of a product and its demand usually have a negative correlation. In short, correlation helps us understand whether two variables move together, move in opposite directions, or are unrelated.\n",
        "13. What does negative correlation mean?\n",
        "    - Negative correlation means that when one variable increases, the other decreases, and vice versa. In other words, the two variables move in opposite directions. The correlation value for such a relationship lies between 0 and -1, where values closer to -1 show a stronger negative relationship. For example, the price of a product and its demand usually have a negative correlation—when the price goes up, demand tends to go down. Similarly, the number of hours spent watching TV and academic performance may also show negative correlation, since more screen time often leads to lower grades.\n",
        "14. How can you find correlation between variables in Python?\n",
        "    - In Python, the correlation between variables is commonly found using the Pandas library with the .corr() function, which computes the correlation coefficient (by default, Pearson correlation) between numeric columns in a DataFrame. For example, creating a DataFrame and calling df.corr() will return a correlation matrix showing the relationship between each pair of variables. Alternatively, NumPy’s corrcoef() can be used for arrays, and for better visualization, libraries like Seaborn can plot a heatmap using sns.heatmap(df.corr(), annot=True) to clearly display the strength and direction of correlations. This makes it easy to analyze how strongly different variables are related to each other.\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    - Causation refers to a cause-and-effect relationship where a change in one variable directly results in a change in another. The key difference between correlation and causation is that correlation only shows an association or relationship between two variables, whereas causation confirms that one variable’s change is the actual reason for the other’s change. For example, ice cream sales and drowning cases often increase together in the summer, showing a positive correlation, but this does not mean ice cream causes drowning. The real cause is hot weather, which increases both swimming (leading to more drowning cases) and ice cream consumption. Thus, correlation shows variables move together, while causation proves one variable directly affects the other.\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "    - An optimizer in Machine Learning and Deep Learning is an algorithm that updates a model’s parameters, such as weights, during training to minimize the loss function and improve performance. The simplest is Gradient Descent, which updates weights step by step in the direction of the negative gradient, but it can be slow for large datasets. Stochastic Gradient Descent (SGD) improves speed by updating weights after each sample or batch, though it can be noisy, while Momentum enhances SGD by adding a momentum term that helps the optimizer move faster and avoid getting stuck in local minima. Adagrad adapts the learning rate for each parameter based on how often it is updated, but its learning rate can shrink too much over time, a problem addressed by RMSprop, which maintains a moving average of squared gradients for stable updates. The most widely used is Adam (Adaptive Moment Estimation), which combines the benefits of Momentum and RMSprop by adapting learning rates and smoothing updates, making it effective for a wide range of deep learning tasks like image recognition and natural language processing.\n",
        "17. What is sklearn.linear_model ?\n",
        "    - sklearn.linear_model is a module in Scikit-Learn that provides various algorithms for building models based on linear relationships between input features and the target variable. It includes methods for both regression and classification tasks, such as LinearRegression for predicting continuous values, LogisticRegression for classification problems, and regularized models like Ridge, Lasso, and ElasticNet to prevent overfitting by penalizing large coefficients. It also offers SGDClassifier and SGDRegressor, which use stochastic gradient descent for handling large-scale datasets efficiently. In short, this module is a collection of tools that allow you to implement linear models effectively for different machine learning tasks.\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - The model.fit() function is used to train a machine learning model by feeding it input features (X) and the corresponding target values (y), allowing the model to learn patterns and adjust its parameters for accurate predictions. In most cases, the two essential arguments are X (independent variables) and y (dependent variable), passed as model.fit(X_train, y_train). Some models may also accept optional arguments like sample_weight to assign different importance to samples, or in deep learning frameworks, parameters such as epochs and batch_size to control the training process. In short, model.fit() is the step where the model learns from data before it can make predictions using .predict().\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "    - The model.predict() function is used to generate predictions from a trained machine learning model. After a model has been fitted with training data using model.fit(), calling .predict() allows us to input new data and obtain the model’s estimated outputs. The only required argument is X, which represents the input features of the data for which predictions are needed. For example, model.predict(X_test) uses the trained model to predict target values for the test dataset. In classification problems, it returns the predicted class labels, while in regression problems, it returns predicted continuous values. In short, model.predict() applies the learning gained during training to new or unseen data to produce outcomes.\n",
        "20. What are continuous and categorical variables?\n",
        "    - Continuous variables are numerical variables that can take an infinite number of values within a given range and are measured rather than counted. They can include fractions or decimals, such as height, weight, temperature, or time. For example, a person’s height could be 160.2 cm or 160.25 cm, showing that values can be very precise.\n",
        "  Categorical variables, on the other hand, represent distinct groups or categories and cannot be measured in a meaningful numerical way. They describe qualities or characteristics and are usually divided into nominal variables (without order, e.g., colors: red, blue, green) and ordinal variables (with order, e.g., education level: high school, graduate, postgraduate).\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling is the process of transforming the values of numerical features into a common scale without distorting differences in their ranges. Since features in a dataset can have very different units (e.g., age in years, income in lakhs, height in cm), scaling ensures that no variable dominates others simply because of its magnitude.\n",
        "\n",
        "It helps in Machine Learning by improving the performance and training speed of models that are sensitive to feature magnitude, such as algorithms based on distance (like K-Nearest Neighbors, K-Means) or gradient descent–based methods (like Linear Regression, Logistic Regression, Neural Networks, and SVMs). Without scaling, features with large ranges can bias the model, leading to poor results. Common scaling techniques include Normalization (scaling values between 0 and 1) and Standardization (scaling values so they have mean 0 and standard deviation 1).\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "    - In Python, scaling can be done using the sklearn.preprocessing module, which provides tools like StandardScaler and MinMaxScaler. StandardScaler standardizes the data by transforming it so that it has a mean of 0 and a standard deviation of 1, making it useful for algorithms that assume normally distributed features such as linear regression or logistic regression. MinMaxScaler, on the other hand, scales all features within a fixed range, usually between 0 and 1, which is helpful for algorithms that rely on distance measurements or neural networks. For example, by importing these scalers from sklearn and applying the fit_transform() method on a dataset, you can easily scale features so that no single variable dominates due to larger numerical values, ensuring fair contribution of all features to the machine learning model.\n",
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in the scikit-learn library that provides a collection of functions and classes to transform and prepare raw data into a suitable format for machine learning models. It includes techniques for scaling (like StandardScaler and MinMaxScaler), encoding categorical variables (like OneHotEncoder and LabelEncoder), normalization, binarization, and even generating polynomial features. Since machine learning algorithms perform better when features are on a similar scale and properly formatted, sklearn.preprocessing helps by cleaning, standardizing, and converting the input data so that models can learn more effectively.\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    - In Python, data is typically split into training and testing sets using the train_test_split function from the sklearn.model_selection module. The training set is used to fit or train the machine learning model, while the testing set is kept aside to evaluate the model’s performance on unseen data. The split is generally done in ratios like 70:30 or 80:20, where the larger portion is assigned to training. For example, by passing the feature variables (X) and the target variable (y) into train_test_split, along with parameters like test_size to specify the percentage for testing and random_state for reproducibility, the dataset is divided into four subsets: X_train, X_test, y_train, and y_test. This approach ensures the model learns patterns from the training data and is then tested fairly on separate data to check its accuracy and generalization.\n",
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of converting categorical data (non-numeric information like labels, text, or categories) into a numerical format that machine learning models can understand and process. Since most algorithms work with numbers rather than text, encoding is an essential preprocessing step. There are different types of encoding techniques: Label Encoding assigns each category a unique integer value, which is simple but can introduce an unintended sense of order between categories; One-Hot Encoding creates binary columns for each category, marking 1 if the category is present and 0 otherwise, which avoids the issue of implying hierarchy but increases the number of features. Other advanced methods like Ordinal Encoding (used when categories have a natural order, e.g., “low,” “medium,” “high”) or Target Encoding (encoding based on the mean of the target variable for each category) are also used depending on the dataset and model type. Thus, data encoding ensures categorical variables are properly represented so that machine learning algorithms can analyze them effectively.\n"
      ],
      "metadata": {
        "id": "eb4X4XWF1Ruz"
      }
    }
  ]
}